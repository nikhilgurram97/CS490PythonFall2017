from sklearn import linear_model
from sklearn import datasets
import numpy as np
import matplotlib.pyplot as p     #Importing necessary libraries

bost=datasets.load_boston()        #Loading boston dataset
b=bost.data[:,np.newaxis,2]         #Creating new dimension so that sklearn library accepts as input without error

xtr=b[:-25]                     #Training x with a small part of the dataset
xts=b[-25:]                     #Testing x with a small part of the dataset

ytr=bost.target[:-25]           #training y with a small part of dataset
yts=bost.target[-25:]           #testing y with a small part of dataset

model=linear_model.LinearRegression()   #initializing linear regression
model.fit(xtr,ytr)                      #Training data with training set
ypred=model.predict(xts)                #predicting data with testing set

p.scatter(xts,yts, color='red')
p.plot(xts,ypred)
p.show()                            #Plotting the line and points

from sklearn.cluster import KMeans
import pandas
import matplotlib.pyplot as p               #importing all necessary libraries

da = pandas.read_csv('cust.csv',header=0)   #reading the CSV Input
d=da.as_matrix(columns=None)                #pandas interpretation to form a numpy array from csv file
k = KMeans(n_clusters=5, random_state=0)    #Initializing KMeans
k.fit(da)                                   #Training data to compute clustering
labels=k.labels_                            #labels array
centroid=k.cluster_centers_                 #centroid points
colors = ["r.","g.","b.","c.","y."]         #colors to differtnate clusters

for i in range(len(d)):
   p.plot(d[i][0],d[i][1],colors[labels[i]],markersize=10)  #plotting points

p.scatter(centroid[:,0],centroid[:,1], marker = "X", s=150, linewidths = 5, zorder =10,color='yellow')     #plotting centroids
p.show()                                                                                    #displaying plot

from sklearn import datasets, metrics
from sklearn import svm
from sklearn.cross_validation import train_test_split   #Importing necessary libraries

i=datasets.load_diabetes()      #Loading diabetes dataset
x=i.data
y=i.target                      #Assigning data and target variables

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)   #Implementing Training an Testing sets, with 20% testing set size of full dataset

m=svm.SVC(kernel="linear")      #Implementing SVM Classification with linear kernel
m.fit(x_train,y_train)          #Training the model
y_pred=m.predict(x_test)        #Predicting data with testing set
a=metrics.accuracy_score(y_test,y_pred)
print("Linear Accuracy: "+str(a))   #Printing Linear Model Accuracy score

m=svm.SVC(kernel="rbf")      #Implementing SVM Classification with RBF kernel
m.fit(x_train,y_train)          #Training the model
y_pred=m.predict(x_test)        #Predicting data with testing set
b=metrics.accuracy_score(y_test,y_pred)
print("RBF Accuracy: "+str(b))   #Printing Linear Model Accuracy score

if (a==b):
    print ("Changing Kernel doesn't change the Accuracy of the model")
else:
    print ("Changing Kernel changed the Accuracy of the model")

    from nltk.tokenize import word_tokenize,sent_tokenize
from nltk.tag import pos_tag
from nltk.stem import WordNetLemmatizer
from nltk import FreqDist

#opening and reading input text file
f=open('text.txt','r')
fr=str(f.read())

#Applying Sentence and Word Tokenization
a=sent_tokenize(fr)
b=word_tokenize(fr)
print'Sentence Tokenizaton:\n'
a1=a[0].split('\n')             #Sentences split into individual elements in a list
print(a1)
print'\n-----\n'
print'Word Tokenizaton:\n'
print b

#Applying POS
e=pos_tag(b)
e1=[v for v in e if v[1] != 'VBG']  #Eliminating Verbs
e2=" ".join([v[0] for v in e1])     #Combining elements of POS to a single sentence
print'\n-----\n'
print'Parts of Speech:\n'
print(e1)
print'\n-----\n'
print'Verb Eliminated Sentence:\n'
print(e2)

#Applying Lemmatization on remaining words
print'\n-----\n'
print'Lemmatization:\n'
l=WordNetLemmatizer()
b1=word_tokenize(e2)
for k in range(len(b1)):
    e1 = l.lemmatize(b1[k])
    print ("The word \'" + b1[k] + "\' in a Lemmatized Version: " + e1)

#Word Frequency of remaining words
fd=FreqDist(e2.split(' '))
print'\n-----\n'
print'Word Frequency:\n'
print(list(fd.most_common(1000)))

#Top 5 repeated words
print'\n-----\n'
print'Top 5 words:\n'
top5=list(fd.most_common(5))
top5d=dict(top5)
top5w=top5d.keys()
print(top5w)

#Finding sentences with most common words
print'\n-----\n'
print'Sentences with most common words:\n'
z1=[]
new=""
def contains_word(s, w):
    return (' ' + w + ' ') in (' ' + s + ' ')   #Function that finds existence of words in a sentence and returns boolean value
for i in range(2):
    for j in range(4):
        x=contains_word(a1[i], top5w[j])
        if (x==1):
            #print("\n"+a1[i])
            z1.append(a1[i])        #Appending the sentence to a lsit
            z=set(z1)               #Converting list to a set
            new=' '.join(z)         #Concatenating into a unique sentence
print(z)
print'\n-----\n'
print'Concatenated Sentences:\n'
print(new)